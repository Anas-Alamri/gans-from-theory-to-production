{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<img src=\"../images/logo.jpg\" style=\"width:85px;height:85px;float:left\" /><h1 style=\"position:relative;float:left;display:inline\">DCGAN in Theory and Practice</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zurutech/gans-from-theory-to-production/blob/master/3.%20TFGAN/3.1.%20DCGAN%20in%20Theory%20and%20Practice.ipynb\">\n",
    "<img align=\"left\" src=\"https://cdn-images-1.medium.com/max/800/1*ZpNn76K98snC9vDiIJ6Ldw.jpeg\"></img>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#Enter-DCGAN\" data-toc-modified-id=\"Enter-DCGAN-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Enter DCGAN</a></span></li><li><span><a href=\"#GANEstimator\" data-toc-modified-id=\"GANEstimator-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>GANEstimator</a></span></li><li><span><a href=\"#Generator:-from-noise-to-insight\" data-toc-modified-id=\"Generator:-from-noise-to-insight-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Generator: from noise to insight</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deconvolution\" data-toc-modified-id=\"Deconvolution-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Deconvolution</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Batch Normalization</a></span></li><li><span><a href=\"#generator_fn()\" data-toc-modified-id=\"generator_fn()-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>generator_fn()</a></span></li></ul></li><li><span><a href=\"#Discriminator\" data-toc-modified-id=\"Discriminator-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Discriminator</a></span><ul class=\"toc-item\"><li><span><a href=\"#discriminator_fn()\" data-toc-modified-id=\"discriminator_fn()-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>discriminator_fn()</a></span></li></ul></li><li><span><a href=\"#Loss-function:-a-bridge-between-two-networks\" data-toc-modified-id=\"Loss-function:-a-bridge-between-two-networks-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Loss function: a bridge between two networks</a></span></li></ul></li><li><span><a href=\"#Practice---Introduction\" data-toc-modified-id=\"Practice---Introduction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Practice - Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-Functions\" data-toc-modified-id=\"Input-Functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Input Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-Images\" data-toc-modified-id=\"Loading-Images-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Loading Images</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#TensorBoard\" data-toc-modified-id=\"TensorBoard-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>TensorBoard</a></span></li></ul></li><li><span><a href=\"#Predictions\" data-toc-modified-id=\"Predictions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#predict_input_fn()\" data-toc-modified-id=\"predict_input_fn()-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>predict_input_fn()</a></span></li></ul></li><li><span><a href=\"#Preparing-for-Production\" data-toc-modified-id=\"Preparing-for-Production-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Preparing for Production</a></span><ul class=\"toc-item\"><li><span><a href=\"#serving_input_receiver_fn\" data-toc-modified-id=\"serving_input_receiver_fn-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>serving_input_receiver_fn</a></span></li><li><span><a href=\"#Exporting-the-model-for-production\" data-toc-modified-id=\"Exporting-the-model-for-production-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Exporting the model for production</a></span></li><li><span><a href=\"#Local-CloudML-Predictions-testing\" data-toc-modified-id=\"Local-CloudML-Predictions-testing-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Local CloudML Predictions testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generate-the-test-noise\" data-toc-modified-id=\"Generate-the-test-noise-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Generate the test noise</a></span></li><li><span><a href=\"#Execute-the-test\" data-toc-modified-id=\"Execute-the-test-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Execute the test</a></span></li></ul></li></ul></li><li><span><a href=\"#To-the-serving\" data-toc-modified-id=\"To-the-serving-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>To the serving</a></span></li><li><span><a href=\"#NOTES\" data-toc-modified-id=\"NOTES-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>NOTES</a></span></li><li><span><a href=\"#Links\" data-toc-modified-id=\"Links-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Links</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow==1.13.1\n",
    "#! pip install tensorflow-gpu==1.13.1\n",
    "! pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import tfgan from contrib\n",
    "tfgan = tf.contrib.gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Enter DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![DCGAN](images/dcgan.png)\n",
    "\n",
    "If you take a look at [this impressive list of GANs [2\\]](#2), you would find out that **DCGAN**, the architecture of our choice, is a small drop in the ocean. While there may be sexier ones, very few offers the same level of clarity, performance and computational efficiency. For these reasons DCGAN is considered one of the cornerstones of this field.\n",
    "\n",
    "Alec Radford, Luke Metz and Soumith Chintala proposed the architecture in [their 2015 paper [3\\]](#3). The idea behind DCGAN is quite straightforward. Combining a set of architectural constraints with the power of CNN yielded a robust, stable and competitive model. Moreover, the architecture is simple: 4 deconvolutional layers for the `Generator` and 4 convolutional layers for the `Discriminator`. The constraints are the following:\n",
    "\n",
    "- All pooling layers are replaced with strided convolutions (discriminator) and fractionally-strided convolutions (generator).\n",
    "- Batch-normalization used in both networks.\n",
    "- Removal of the fully-connected layers (except for the discriminator output).\n",
    "- `ReLU` for all Generator layers except the output, which uses `tanh`.\n",
    "- `LeakyReLU` activation in the discriminator for all layers.\n",
    "\n",
    "Recently there have been some advancements in state of the art (e.g., Spectral Normalization). However, it is essential to have a firm understanding of the basic concepts. So, we leave you (an opinionated) list of further resources at the end of this notebook, to get you up to speed with the most exciting researches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### GANEstimator\n",
    "We are going to create all that we need for the `GANEstimator` object. The signature is the following:\n",
    "\n",
    "```python\n",
    "__init__(\n",
    "    model_dir=None,\n",
    "    generator_fn=None,\n",
    "    discriminator_fn=None,\n",
    "    generator_loss_fn=None,\n",
    "    discriminator_loss_fn=None,\n",
    "    generator_optimizer=None,\n",
    "    discriminator_optimizer=None,\n",
    "    get_hooks_fn=None,\n",
    "    get_eval_metric_ops_fn=None,\n",
    "    add_summaries=None,\n",
    "    use_loss_summaries=True,\n",
    "    config=None,\n",
    "    warm_start_from=None,\n",
    "    is_chief=True\n",
    ")\n",
    "```\n",
    "With this in mind, we are going to create the `generator_fn` and the `discriminator_fn`. These two functions represent the `Generator` and the `Discriminator`, respectively. \n",
    "Finally, we are going to set to the `GANEstimator` all the other parameters on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Generator: from noise to insight\n",
    "\n",
    "![DCGAN Generator](images/dcgan_generator.png)\n",
    "\n",
    "Recalling the theoretical explanation, the Generator is the network responsible for the data-generation. It learns how to fool the discriminator, so it learns how to produce realistic results. Those results are then \"sampled\" from the learned manifold.\n",
    "\n",
    "The most common type of generator input is noise (i.e.: random values). More specialized GANs may require extra parameters. Since our full-demo uses a Deep Convolutional GAN (DCGAN), we don't need any other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Deconvolution\n",
    "\n",
    "Intuitively, the idea behind this operation is the following:\n",
    "\n",
    "![Deconvolution](images/deconvolution.png)\n",
    "\n",
    "\n",
    "> When neural networks generate images, they build the images starting from low-resolution high-level descriptions. In this way, the network starts describing a \n",
    "> rough representation and then fill in the details to create the final image.\n",
    ">\n",
    ">To do this, we need some way to go from a lower resolution image to a higher one. We generally do this with the deconvolution operation. \n",
    ">Roughly, deconvolution layers allow the model to use the points from the small image to “paint” a larger area in the bigger output image.\n",
    "\n",
    "In practice, the deconvolution operation is often implemented by resizing, using bi-linear or nearest neighbor interpolation, followed by a convolution operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "What you need to know about batch normalization is that is a layer that normalizes the values. TensorFlow makes it very easy to implement such operation:  \n",
    "\n",
    "```python\n",
    "tf.keras.layers.BatchNormalization()\n",
    "```\n",
    "\n",
    "The benefit of using batch normalization has been extensively discussed and proved in various papers. We do not enter into the theoretical depth of BatchNormalization, but you can refer to [[3\\]](#3), [[8\\]](#8) and [[9\\]](#9) to learn more about the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### generator_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def deconv2d(inputs, filters, strides=(1, 1), activation=tf.nn.relu):\n",
    "    \"\"\"\\\"Deconvolution\\\" layer.\n",
    "    \n",
    "    It uses upsampling with nearest neighbor interpolation to reduce the\n",
    "    presence of checkerboard artifacts.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_h, input_w = inputs.shape[1].value, inputs.shape[2].value\n",
    "    layer_1 = tf.image.resize_nearest_neighbor(\n",
    "        inputs, (2 * input_h, 2 * input_w), name=\"NNUpSample2D\"\n",
    "    )\n",
    "    # Padding before convolution is used to reduce boundary artifacts\n",
    "    layer_1 = tf.pad(layer_1, [[0, 0], [2, 2], [2, 2], [0, 0]], mode=\"CONSTANT\")\n",
    "    layer_2 = tf.layers.conv2d(\n",
    "        inputs=layer_1,\n",
    "        filters=filters,\n",
    "        kernel_size=5,\n",
    "        padding=\"valid\",\n",
    "        use_bias=False,\n",
    "        activation=activation,\n",
    "        strides=strides,\n",
    "    )\n",
    "    return layer_2\n",
    "\n",
    "def generator_fn(inputs, mode):\n",
    "    \"\"\"Generator producing images from noise.\n",
    "\n",
    "        Args:\n",
    "            inputs: A single Tensor representing noise.\n",
    "            mode: tf.estimator.ModeKeys\n",
    "\n",
    "        Returns:\n",
    "            A 64x64 (None, 4096) flattened tensor whose values are\n",
    "            inside the (-1, 1) range.\"\"\"\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    linear = tf.layers.dense(inputs=inputs, units=1024 * 4 * 4, activation=tf.nn.relu)\n",
    "    net = tf.reshape(linear, (-1, 4, 4, 1024))\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = deconv2d(net, 512)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = deconv2d(net, 256)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = deconv2d(net, 128)\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = deconv2d(net, 64)\n",
    "    net = tf.layers.conv2d(\n",
    "        inputs=net,\n",
    "        filters=3,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        data_format=\"channels_last\",\n",
    "        use_bias=False,\n",
    "        strides=(1, 1),)\n",
    "    output = tf.tanh(net)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Discriminator\n",
    "\n",
    "![DCGAN Discriminator](images/dcgan_discriminator.png)\n",
    "\n",
    "As for the `input_fn()` we can reuse all the code we defined for the vanilla `Estimator` describing the discriminator network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### discriminator_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def custom_conv2d(inputs, filters, strides=(2,2)):\n",
    "    \"\"\"Helper layer used to instantiate `tf.layers.conv2d` with proper arguments.\"\"\"\n",
    "    layer_1 = tf.layers.conv2d(\n",
    "        inputs=inputs,\n",
    "        filters=filters,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        data_format=\"channels_last\",\n",
    "        use_bias=False,\n",
    "        strides=strides,\n",
    "    )\n",
    "    layer_1 = tf.nn.leaky_relu(layer_1, alpha=0.2)\n",
    "\n",
    "    return layer_1\n",
    "\n",
    "def discriminator_fn(inputs, conditioning, mode):\n",
    "    \"\"\"Build the Discriminator network.\n",
    "    Args:\n",
    "        inputs: a batch of images to classify, expected input shape (None, 64, 64 , 3)\n",
    "        conditioning: a batch of labels, it is used for conditioning in the some model (es Conditional GAN).\n",
    "            GANEstimator wants this parameters around, just define an arguments so that discriminator_fn is not broken.\n",
    "        mode: tf.estimator.ModeKey\n",
    "    \n",
    "    Returns:\n",
    "            The output (logits) of the discriminator.\n",
    "    \"\"\"\n",
    "    \n",
    "    # In every mode, define the model\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    net = custom_conv2d(inputs, filters=64) # <--64*64*3\n",
    "    net = custom_conv2d(net, filters=128) # <-- 32*32*64   \n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = custom_conv2d(net, filters=256) # <-- 16*16*128\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = custom_conv2d(net, filters=512) # <-- 8*8*256\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = custom_conv2d(net, filters=1024) # <-- 4*4*512\n",
    "    net = tf.layers.batch_normalization(net, training=is_training)\n",
    "    net = tf.reshape(net, (-1, net.shape[1] * net.shape[2] * net.shape[3]))\n",
    "    output = tf.layers.dense(net, units=1) # <-- 2*2*1024 --> 1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Loss function: a bridge between two networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "There are a lot of loss functions usable in GANs' architectures. From very domain-specific ones to others that are perfect for general use cases. Since our goal here falls in the latter category, we use the so-called **Non-Saturating Loss** which is the non-saturating variant of the **MinMax Loss** proposed by Goodfellow in the [original paper [1\\]](#1).\n",
    "\n",
    "As stated above, one of `TFGAN`'s beauties is its offer of ready-to-use losses. If you cannot find the loss you want, you can create your own. \n",
    "\n",
    "We use the following two losses:\n",
    "\n",
    "```python\n",
    "generator_loss_fn=tfgan.losses.minimax_generator_loss\n",
    "\n",
    "discriminator_loss_fn=tfgan.losses.minimax_discriminator_loss\n",
    "```\n",
    "\n",
    "It is that easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Practice - Introduction\n",
    "\n",
    "Having seen the general structure of the TFGAN library, we dive right into the model architecture.\n",
    "\n",
    "In our showcasing of the TensorFlow API, we have built an image recognition network. GANs, however, need the training of both a Generator and a Discriminator together.\n",
    "\n",
    "This task, in the case of writing it in *vanilla TensorFlow*, turns out to be verbose and performance-constrained.\n",
    "\n",
    "On the other hand, in the case of *Estimator API* results in somewhat complicated.\n",
    "\n",
    "Thanks to *GANEstimator*, instead, it becomes remarkably simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Input Functions\n",
    "A `GANEstimator` object it is used to train the GAN models. The `GANEstimator` `train` function takes as a parameter an \"input function\" (i.e.: `input_fn`). We have to write that `input_fn` in a specific manner. The way we need to write that function is pretty much the same as the one we would define for a standard `tf.Estimator`. The most important thing is its *return value* that should be one of the following:\n",
    "\n",
    "> A `tf.data.Dataset` object: Outputs of Dataset object must be a tuple (features, labels) with the same constraints as below.\n",
    "> \n",
    "> A tuple (features, labels): Where `features` is a Tensor or a dictionary of string feature name to Tensor and `labels` is a Tensor or a dictionary of string label name to Tensor. [...]\n",
    "\n",
    "However, GANs training (and evaluating) needs to use two types of features: the fake ones (noise) and the real ones (real images). On the other hand, as we have just said, we need to be compatible with the standard `tf.Estimator` return \"signature\", i.e.: (features, labels). Exploiting this return signature, we can achieve our goal. Hence, instead of returning a (features, labels) couple we return a (noise, real) couple. In practice, what we need to do is the following:\n",
    "\n",
    "1. Structure the `input_fn` code as a **function object** as requested by `GANEstimator.train` (and `.evaluate`).\n",
    "2. Return **noise** first and then the **real data** (the images) instead of (features, labels) couple. Optionally we can return the labels too.\n",
    "\n",
    "During the training, the `GANEstimator` object will internally manage the training process and automatically pass the needed values to the `generator_fn` and `discriminator_fn` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Loading Images\n",
    "\n",
    "We use the Celeb-A dataset downloaded using tensorflow-datasets (tfds) to easily create the input function that satisfies all the requirements.\n",
    "\n",
    "![DCGAN Generator](images/celeba_sample_imgs.png)\n",
    "\n",
    "The image is an example of the CelebA Dataset, source: [here](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def get_train_input_fn(batch_size, num_epochs, noise_dim):\n",
    "    \"\"\"The input function that builds the `tf.data.Dataset` object and instantiate\n",
    "    the iterator correctly ready to be use.\n",
    "    Returns:\n",
    "        the iterator associated to the built Dataset object.\n",
    "    \"\"\"\n",
    "    \n",
    "    def convert_and_resize(features):\n",
    "        image = tf.image.convert_image_dtype(features[\"image\"], dtype=tf.float32)\n",
    "        image = (image - 0.5) * 2\n",
    "        image = tf.image.resize(image, size=(64, 64))\n",
    "        features[\"image\"] = image\n",
    "        return features\n",
    "\n",
    "    def _input_fn():\n",
    "        real_data = tfds.load(\"celeb_a\",split=tfds.Split.ALL)\n",
    "        real_data = real_data.map(convert_and_resize).map(lambda feature: feature[\"image\"])\n",
    "        real_data = real_data.batch(batch_size, drop_remainder=True).repeat(num_epochs)\n",
    "        real_data_iterator = real_data.make_one_shot_iterator()\n",
    "        \n",
    "        noise = tf.random_normal([batch_size, noise_dim], name=\"train_noise\")\n",
    "        real_batch = real_data_iterator.get_next()\n",
    "        real_batch.set_shape((batch_size,) + tuple(real_batch.shape[1:]))\n",
    "        return noise, real_batch\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def dcgan():\n",
    "    \n",
    "    # Hyperparameters\n",
    "    model_dir = \"../logs/\"\n",
    "    batch_size = 128\n",
    "    num_epochs = 1\n",
    "    noise_dim = 100\n",
    "    \n",
    "    # Run Configuration (it has other arguments)\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        model_dir=model_dir, save_summary_steps=50, save_checkpoints_steps=500)\n",
    "    \n",
    "    # Instantiate the GANEstimator object\n",
    "    gan_estimator = tfgan.estimator.GANEstimator(\n",
    "        config=run_config,\n",
    "        generator_fn=generator_fn,\n",
    "        discriminator_fn=discriminator_fn,\n",
    "        generator_loss_fn=tfgan.losses.modified_generator_loss,\n",
    "        discriminator_loss_fn=tfgan.losses.modified_discriminator_loss,\n",
    "        generator_optimizer=tf.train.AdamOptimizer(0.0002, 0.5),\n",
    "        discriminator_optimizer=tf.train.AdamOptimizer(0.0002, 0.5),\n",
    "        add_summaries=tfgan.estimator.SummaryType.IMAGES\n",
    "    )\n",
    "    \n",
    "    # Instantiate the train_input_fn\n",
    "    # The model will train until it exhausts the Dataset which is repeated EPOCH times\n",
    "    train_input_fn = get_train_input_fn(batch_size, num_epochs, noise_dim)\n",
    "    trained_model = gan_estimator.train(train_input_fn, max_steps=None)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### TensorBoard\n",
    "\n",
    "In order to track our training we need to launch a **TensorBoard** session pointing to the folder (`model_dir`) containing the logs generated by our training.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=PATH_TO_YOUR_MODEL_DIR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_model = dcgan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Predictions\n",
    "\n",
    "We train Machine Learning models because we want them to perform some specific task since we now have our GAN trained, we can finally use it to \"predict\" AKA generate a new image from a noise vector.\n",
    "\n",
    "Once again, TFGAN makes it as easy as invoking the `predict()`  method of our `GANEstimator` while passing to it a `predict_input_fn` as a required argument. \n",
    "\n",
    "### predict_input_fn()\n",
    "\n",
    "As previously mentioned while theoretically identical, `train_input_fn` and `predict_nput_fn()` should be implemented differently.  The first one is a simple `tf.random_normal()` node, the second should make proper use of the `tf.Dataset` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def _predict_input_fn(batch_size, noise_dims=100, **kwargs):\n",
    "    \n",
    "    def predict_input_fn():\n",
    "        noise_gen = np.array([np.float32(np.random.normal(size=[1, noise_dims])) for i in range(batch_size)])\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(noise_gen)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "        \n",
    "    return predict_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_batch = 3\n",
    "predict_input_fn = _predict_input_fn(batch_size=predictions_batch)\n",
    "predictions = trained_model.predict(predict_input_fn)\n",
    "[next(predictions) for _ in range(predictions_batch)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Preparing for Production\n",
    "\n",
    "Now that we have our model trained and ready we are just a couple steps away from being able to put our model into production.\n",
    "\n",
    "### serving_input_receiver_fn\n",
    "\n",
    "To serve a model in production, we first need to equip it with an interface which will receive data from our requests, such interface is specified by the aptly named `serving_input_receiver_fn`. Of the three input functions, this is the trickiest one since it has its peculiar API.\n",
    "\n",
    "This functions requires its output to be a either a `ServingInputReceiver` or a `TensorServingInputReceiver` object; the documentation on their use is clear:\n",
    "\n",
    "> The normal `ServingInputReceiver` always returns a feature dict, even if it contains only one entry, and so can be used only with models that accept such a dict. \n",
    ">For models that accept only a single raw feature, the `serving_input_receiver_fn` provided to `Estimator.export_savedmodel()` should return this `TensorServingInputReceiver`.\n",
    "\n",
    "Since our model needs only a noise vector to get going, we can use `TensorServingInputReceiver`.\n",
    "\n",
    ">The expected return values are: \n",
    "> - **features**: A single `Tensor` or `SparseTensor`, representing the feature to be passed to the model. \n",
    "> - **receiver_tensors**: A Tensor, SparseTensor, or dict of string to Tensor or SparseTensor, specifying input nodes where this receiver expects to be fed by default. Typically, this is a single placeholder expecting serialized `tf.Example` protos. \n",
    "> - **receiver_tensors_alternatives**: a dict of string to additional groups of receiver tensors, each of which may be a `Tensor`, `SparseTensor`, or dict of string to `Tensor` or `SparseTensor`. These named receiver tensor alternatives generate additional serving signatures, which may be used to feed inputs at different points within the input receiver subgraph. A typical usage is to allow feeding raw feature Tensors downstream of the `tf.parse_example()` op. Defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def _serving_input_receiver_fn():\n",
    "    \"\"\"Instantiate a placeholder for our serving input data.\n",
    "\n",
    "    Call the custom function `serving_input_fn()`, built following\n",
    "    TensorFlow Estimator API convention, initializing the placeholder for\n",
    "    the noise we will feed the model during its serving.\n",
    "\n",
    "    The Serving Input function has two key elements:\n",
    "\n",
    "        - the data-processing step, where we concretely prepare data to be\n",
    "        fed to the:\n",
    "        - Placeholder, it is the node where the input are fed.\n",
    "\n",
    "    The things to notice is that while using `ServingInputReceiver`\n",
    "    your data processing step should have at its core the parsing of the\n",
    "    tf.Example received.\n",
    "\n",
    "    With `TensorServingInputReceiver` our data won't really be passed by the\n",
    "    request, instead it will have to be generated ''model-side'' inside the\n",
    "    `_serving_input_receiver_fn` itself.\n",
    "\n",
    "    Returns:\n",
    "        tf.estimator.export.TensorServingInputReceiver passing the\n",
    "        placeholder for the noise to it.\n",
    "    \"\"\"\n",
    "\n",
    "    receiver_tensors = tf.placeholder(shape=[None, 100], dtype=tf.float32, name=\"serving_noise\")\n",
    "    return tf.estimator.export.TensorServingInputReceiver(receiver_tensors, receiver_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Exporting the model for production\n",
    "\n",
    "Exporting the model is as easy as calling `GANEstimator.export_savedmodel()` which the same as the normal `Estimator`.\n",
    "\n",
    "> This method builds a new graph by first calling the serving_input_receiver_fn to obtain feature Tensors, and then calling this Estimator's model_fn to generate the model graph based on those features. It restores the given checkpoint (or, lacking that, the most recent checkpoint) into this graph in a fresh session. Finally it creates a timestamped export directory below the given export_dir_base, and writes a SavedModel into it containing a single MetaGraphDef saved from this session.\n",
    "\n",
    "We have to specify an output folder and the `serving_input_receiver_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "export_dir_base = \"../assets/exported_models\"\n",
    "trained_model.export_savedmodel(\n",
    "    export_dir_base, _serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Local CloudML Predictions testing\n",
    "\n",
    "Before going over the required steps for model serving, we want to test it locally to make sure that the exported models will behave correctly once loaded onto CloudML Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Generate the test noise\n",
    "\n",
    "Google Cloud often uses Newline Delimited JSON  when working with JSON-formatted data; we can use the `jsonlines` Python library assure our compliance with the standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "NOISE_DIMS = 100\n",
    "\n",
    "with jsonlines.open(\"../assets/test_noise.ndjson\", \"w\") as writer:\n",
    "    noise = np.random.normal(size=(BATCH_SIZE, NOISE_DIMS)).tolist()\n",
    "    noised_dict = [{\"input\": n} for n in noise]\n",
    "    writer.write_all(noised_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Execute the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_DIR=\"../assets/exported_models\"\n",
    "MODEL_ID=\"1535124551\"\n",
    "JSON_INSTANCES=\"../assets/test_noise.ndjson\"\n",
    "\n",
    "rm ~/google-cloud-sdk/lib/googlecloudsdk/command_lib/ml_engine/*.pyc\n",
    "\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=$MODEL_DIR/$MODEL_ID \\\n",
    "    --json-instances=$JSON_INSTANCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## To the serving\n",
    "\n",
    "Now that we have the exported model we are ready to finally serve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## NOTES\n",
    "\n",
    "[1]: [The GAN Landscape: Losses, Architectures, Regularization, and Normalization](https://arxiv.org/abs/1807.04720v1), which in our opinion is one of the most thorough scientific studies on GANs out there, suggests that `Spectral Normalization` is the real big deal and that `BatchNorm` actually hurt performance if applied to the Discriminator. We still decided to go with the classic formulation of DCGAN as not to overtax you with theoretical discussions. We leave the implementation of the `Spectral Normalization`(and SNGAN) to you as an exercise. On the theoretical treating of `BatchNorm` we recommend a back to back reading of [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167), [Understanding Batch Normalization](https://arxiv.org/abs/1806.02375v1), [How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)](https://arxiv.org/abs/1805.11604) \n",
    "\n",
    "[2]: While often you want to work with **features (and labels)** fetched from a `tf.Dataset`, the **noise** should always be instantiated using a simple TensorFlow node. Trying to create a noise-containing `tf.Dataset` for the `train_input_fn` is not worth the effort. NOTE: that as things are different for the [predict_input_fn](##predict_input_fn()). \n",
    "\n",
    "[3]: See [Tensorflow 2.0: Keras is not (yet) a simplified interface to Tensorflow[10\\]](#10) to have an understanding on the compatibility between Keras and Tensforflow (2.0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Links\n",
    "\n",
    "<a id=\"1\">[1]</a>: Generative Adversarial Networks : https://arxiv.org/abs/1406.2661\n",
    "\n",
    "<a id=\"2\">[2]</a>: really-awesome-gan : https://github.com/nightrome/really-awesome-gan\n",
    "\n",
    "<a id=\"3\">[3]</a>: DCGAN : https://arxiv.org/abs/1511.06434\n",
    "\n",
    "<a id=\"4\">[4]</a>: Deconvolution and Checkerboard Artifacts : https://distill.pub/2016/deconv-checkerboard/\n",
    "\n",
    "<a id=\"5\">[5]</a>: TFGAN MNIST GAN Example : https://github.com/tensorflow/models/tree/master/research/gan/mnist_estimator\n",
    "\n",
    "<a id=\"6\">[6]</a>: Estimator API : https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\n",
    "\n",
    "<a id=\"7\">[7]</a>: export_savedmodel : https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel\n",
    "\n",
    "<a id=\"8\">[8]</a>: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift : https://arxiv.org/abs/1502.03167\n",
    "\n",
    "<a id=\"9\">[9]</a>: How Does Batch Normalization Help Optimization? : https://arxiv.org/abs/1805.11604\n",
    "\n",
    "<a id=\"9\">[10]</a>: Tensorflow 2.0: Keras is not (yet) a simplified interface to Tensorflow : https://pgaleone.eu/tensorflow/keras/2019/01/19/keras-not-yet-interface-to-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
